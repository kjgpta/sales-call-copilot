# Sales Call Copilot

## Overview
Sales Call Copilot is designed to process sales-call transcripts and answer user queries by leveraging a language model. The system uploads transcripts, extracts relevant segments, and utilizes both a large language model (LLM) and an extractive fallback method to generate answers with source references.

## Architecture & Workflow
1. **Transcript Uploading & Listing**  
   - A frontend (HTML + Tailwind CSS) allows users to upload transcripts and list available calls.
   - The backend (not shown in this repository) provides endpoints such as `/api/ingest-sample`, `/api/upload`, and `/api/ask`.

2. **Query Answering Process**  
   - When a query is submitted, the system gathers context segments from the uploaded transcripts.
   - The function `answer_query` in `llm.py` composes a prompt using the user query and the formatted transcript contexts.

3. **LLM Integration**  
   - If the `LLM_PROVIDER` is set to `openai` and a valid `OPENAI_API_KEY` exists, the code calls the external OpenAI API.
   - A custom prompt is created using templates from `QA_SYSTEM_PROMPT` and `QA_USER_TEMPLATE`.
   - The OpenAI model (here using the identifier "o4-mini-2025-04-16") generates an answer.
   - To ensure transparency, the generated answer must include a "Sources:" block.

4. **Extractive Fallback**  
   - If the LLM API call fails (or if LLM is not available), the module falls back to an extractive approach.
   - The extractive method collects the top transcript segments that best match the query.
   - A sources list is generated by `_sources_block` function to provide references alongside the answer.

## Code Choices
- **Separation of Concerns:**  
  Functions like `_format_context` and `_sources_block` modularize data formatting. This keeps the code clean and allows for easy adjustments.
  
- **Error Handling:**  
  The `try/except` block ensures that if the LLM call fails, users still receive an answer via the fallback method.
  
- **User Feedback:**  
  The answer always includes explicit sources to provide context and build trust with users regarding the origin of the provided information.

## File Structure
- **copilot/core/llm.py:**  
  Contains core logic for integrating with the LLM, formatting responses, and managing fallbacks.
- **Frontend files (e.g., index.html):**  
  Handle user interactions, file uploads, and displaying transcript data.

## Workflow Summary
1. **Upload & Ingest:** Transcripts are uploaded and ingested into the system.
2. **User Query:** The user submits a question via the frontend.
3. **Answer Generation:**  
   - The system gathers relevant transcript segments.
   - It then calls the LLM to generate an answer or falls back to an extractive approach.
   - The answer is returned with a detailed "Sources:" section.
4. **Client Render:** The frontend displays the answer and the relevant source segments for review.

## Running the Project
- Configure the LLM settings (`LLM_PROVIDER`, `OPENAI_API_KEY`) in the configuration file.
- Start the backend server (e.g., using `uvicorn server.app:app --reload`).
- Open the frontend HTML file in a browser and interact with the system.

## Conclusion
The design ensures a robust and transparent workflow from transcript ingestion to query answering, providing users with both AI-generated insights and traceable sources.
    storage.py
    parser.py
    chunker.py
    index_lex.py
    index_dense.py
    retriever.py
    prompts.py
    llm.py
    composer.py
  data/                # sample transcripts (already copied)
  index/               # built indices (bm25.pkl, embeddings.npy, mappings)
  tests/
    test_parser.py
```

## Notes
- BM25 is rebuilt on each ingestion batch (simple + robust).
- Dense embeddings are optional; if disabled, system runs lexical-only.
- This repo focuses on the **grounding/citation discipline** and a clean baseline. Swap models as needed.


## New features
- **OpenAI by default** (`LLM_PROVIDER=openai` in `.env.example`) — set `OPENAI_API_KEY`.

- **Cross-encoder re-ranking** (if `sentence-transformers` available). Configure via `RERANKER` & `RERANKER_MODEL`.

- **Sentiment & topic filters** during ingestion and search:

  ```bash

  # Include pricing-related segments only

  python copilot/cli.py ask "What did we agree on for discounts?" --include-topics pricing,discount


  # Exclude competitor chatter, restrict to negative sentiment

  python copilot/cli.py ask "Any concerns about uptime?" --exclude-topics competitor --min-sentiment -1.0 --max-sentiment -0.2
  ```

- **Citation coalescing** merges adjacent segments from the same call into cleaner `[MM:SS–MM:SS]` windows.



## Web UI (FastAPI)
Run the API + static UI:
```bash
uvicorn server.app:app --reload
# open http://127.0.0.1:8000
```
In the UI:
- Click **Ingest Sample Data** to parse the transcripts in `./data` and build indices.
- Ask questions with optional topic/sentiment filters.
- Answers show a **Sources** section; the UI renders source cards with `[MM:SS–MM:SS]` windows.


### Upload your own transcripts & deep links
- Drag & drop `.txt` files in the UI or click **Choose files** (hit **Ingest Sample Data** if you want the provided examples).
- Click **Open in Viewer** on any source card to jump to the exact timestamp range in the transcript viewer.
- You can also deep link directly: `/?call_id=2_pricing_call&start=01:24&end=01:47`.

### Code
The code is available on [GitHub](https://github.com/kjgpta/sales-call-copilot).